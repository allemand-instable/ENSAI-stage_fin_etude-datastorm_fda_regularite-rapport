\chapter{M√©thodologie}
\minitoc%

\section{Donn√©es Fonctionnelles : l'essentiel}
    \subsection{G√©n√©ralit√©s sur les donn√©es fonctionnelles}

    \input{content/chapter_2/fda.tex}

    \subsection{Cas non ind√©pendant : s√©ries temporelles de donn√©es fonctionnelles}
        
    \input{content/chapter_2/fda_ts.tex}

\section{Estimation de la r√©gularit√© locale des trajectoires}

\subsection{Ce qu'on entend par r√©gularit√© locale}

\input{content/chapter_2/regularite_locale.tex}


\subsection{Estimation des param√®tres r√©gularit√© locale des trajectoire}

\subsubsection{Deux m√©thodes d'obtention de la r√©gularit√© locale des trajectoires}

Il existe deux m√©thodes diff√©rentes pour estimer la r√©gularit√© des trajectoires. Si la cl√© des deux m√©thodes pour extraire la r√©gularit√© locale est le th√©or√®me de continuit√© de Kolmogorov √©nonc√© ci-dessous, les deux m√©thodes diff√®rent par les points $t \in \mathcal T$ consid√©r√©s dans l'estimation des accroissements quadratiques $\esperance{ \vert X(u) - X(v) \vert^2 }$ utilis√©s pour l'estimation de la r√©gularit√© locale. 

\begin{thm}[Continuit√© de Kolmogorov]
    \emph{r√©f√©rence : } ~\cite[thm : 2.197 | page : 145]{capasso2015introduction}

    $$
    \begin{array}{ll}
        \textsf{\faCaretSquareRight} 
        & X : \, \begin{array}{ccc}
            \mathbb{R_+} \times \Omega & \longrightarrow & \mathbb{R} \\
            (t, \omega) & \longmapsto & X(t, \omega) = x(t)
            \end{array} \textsf{s√©parable}
        & &
        \\
        \textsf{\faCaretSquareRight}
        & \exists r,c, \varepsilon, \delta \in \mathds R_+ \quad (\forall h < \delta)(\forall t \in \mathds R_+)  \quad \esperance{ | X(t+h) - X(t) |^r } \leq c\cdot h^{1+\varepsilon}
    \end{array}
    $$
    \begin{center}
        $\Downarrow$
    \end{center}
    $$ 
    \textbf{\faIcon{asterisk}}\, \boxed{
        X \textsf{ est continu en } t \in \mathds R_+ \textsf{ pour presque tout } \omega \in \Omega
    }
    $$
    \begin{center}
        ie : il existe une version $\tilde X$ de $X$ continue en $t$ telle que $\proba{ \tilde X(t) = X(t)} = 1$
    \end{center}

    $$
    \textbf{\faIcon{asterisk}} \, \boxed{
        \tilde{X} \textsf{ est } \gamma \textsf{-H√∂lderienne en } t  \textsf{ pour tout } 0 < \gamma < \frac{\varepsilon}{r}
    }
    $$
\end{thm}

Etant donn√© que notre estimateur utilise les incr√©ments quadratiques, on se place dans le cas o√π $r = 2$.

\editorwarn{Mettre la version avec H√∂lder, qui permet de d√©river des fda la r√©gularit√© locale}


La m√©thode de Golovkine et al. ~\cite[pages : 7‚Äî9]{golovkineRegularityOnlineEstimationNoisyCurve} n'utilise que les points observ√©s, et construit un estimateur des incr√©ments quadratiques √† base de statistique d'ordre. 

$$\theta( T_{(l)}, T_{(k)}) = \esperance{ \left| X( T_{(l)}) - X(T_{(k)}) \right|^2 }  \begin{align*}
    &\quad \underset {\textsf{LGN}} \approx 
    & \boxed{\frac 1 {N} \sum\limits_{n=1}^N \left| \statrang Y n {2k-1} \statrang Y n k \right|^2 \isdef \hat \theta_k}
    \\
    & \, \underset {+ \mathcal C^0 Kol.} {\overset {\textsf{H√∂lder}} \approx}
    & L_{t_0} \esperance{| \ordered T l - \ordered T k |^{2H_{t_0}}} 
    & \rightarrow H_{t_0} = f(\theta)
\end{align*}  $$

$$\hat H_{t_0}(k) = \begin{cases} \displaystyle\frac{\log\left( \hat \theta_{4k-3} - \hat \theta_{2k-1}  \right) - \log \left(  \hat\theta_{2k-1} - \hat \theta_k \right)}{2\log 2} & \hat \theta_{4k-3} > \hat \theta_{2k-1} > \hat \theta_{k}
    \\
    1 & \textsf{sinon}
\end{cases}$$

\info{Cette m√©thode peut s'av√©rer sp√©cifiquement utile lorsque l'on traite un flux de donn√©es, car l'arriv√©e de nouvelles donn√©es ne n√©cessite pas sp√©cifiquement de recalculer les incr√©ments quadratiques sur l'ensemble des points observ√©s. }

L'autre m√©thode propos√©e par ~\cite{golovkine2021adaptive,maissoro-SmoothnessFTSweakDep}, elle se base sur l'utilisation de points non observ√©s, inf√©r√©s par lissage des courbes, √† une distance $\Delta / 2$ les uns des autres pour estimer les incr√©ments quadratiques. Cette derni√®re m√©thode implique le choix d'un hyper-param√®tre lors de l'estimation $\Delta$ et pourrait √™tre sensible √† la qualit√© du lissage de la courbe. Etant donn√© que l'objectif de la d√©termination de la r√©gularit√© locale est de pouvoir faire un lissage √† noyaux adaptatif en fonction de l'objet que l'on souhaite estimer, on appelle le lissage effectu√© pour estimer la r√©gularit√© \og pr√©-lissage \fg.

% https://tex.stackexchange.com/questions/156993/plotting-weierstrass-function
\begin{center}
\begin{tikzpicture}
    \pgfmathsetmacro{\pgfdeltavalue}{0.25}
    \pgfmathsetmacro{\pgftvalue}{0.4}
    \pgfmathsetmacro{\pgfarrowheight}{-0.25}
    \pgfmathsetmacro{\pgfarrowfrom}{\pgftvalue - \pgfdeltavalue/2}
    \pgfmathsetmacro{\pgfarrowto}{\pgftvalue + \pgfdeltavalue/2}
    \begin{axis}[axis lines=middle,
        xmin=0, xmax=1,
        ymin=-0.4, ymax=0.4,
        axis equal image,
        ytick=\empty,
        xtick=\empty,
    ]
    \addplot [flatuicolors_green, samples=300, domain=0:1.1] {weierstrass(2*x,2,15)};

    \addplot [flatuicolors_imperial, mark=*, only marks] coordinates {(\pgftvalue, {weierstrass(2*0.4,2,15)})};
    \addplot [flatuicolors_imperial, mark=*, only marks] coordinates {(\pgftvalue - \pgfdeltavalue/2, {weierstrass(2*(\pgftvalue - \pgfdeltavalue/2),2,15)})};
    \addplot [flatuicolors_imperial, mark=*, only marks] coordinates {(\pgftvalue + \pgfdeltavalue/2, {weierstrass(2*( \pgftvalue + \pgfdeltavalue/2 ),2,15)})};

    \addplot[color=flatuicolors_imperial,mark=none, thick, dashed] (\pgftvalue + \pgfdeltavalue/2,x);
    \addplot[color=flatuicolors_imperial,mark=none, thick, dashed] (\pgftvalue - \pgfdeltavalue/2,x);
    \addplot[color=flatuicolors_imperial,mark=none, thick, dashed] (\pgftvalue,x);

    \node at (axis cs: \pgftvalue, \pgfarrowheight - 0.05) {$\colorize[flatuicolors_aqua]{ \mathbf \Delta}$};

    \draw[flatuicolors_aqua, ->] (axis cs:\pgfarrowfrom, \pgfarrowheight) -- (axis cs: \pgfarrowto, \pgfarrowheight);
    \draw[flatuicolors_aqua, <-] (axis cs:\pgfarrowfrom, \pgfarrowheight) -- (axis cs: \pgfarrowto, \pgfarrowheight);

    \end{axis}

\end{tikzpicture}
\end{center}
\smallskip

soit $\Delta \in \mathds R_+^*$, on pose $t_1 \isdef t_0 - \frac \Delta 2$ et $t_3 \isdef t_0 + \frac \Delta 2$.

alors on approche $\theta (t_1,t_3) = \esperance{ \left| X(t_3) - X(t_1) \right|^2 } = \theta_{13}$ par :

$$\tilde \theta_{13} = \frac 1 N \sum\limits_{n=1}^N \left| X(t_3) - X(t_1) |^2$$

qui n'est pas observable, √©tant donn√© qu'il n'est pas garanti d'observer $X(t_1)$ et $X(t_3)$, et qu'il faut donc lisser dans un premier temps les courbes pour pouvoir √©valuer $X$ en $t_1$ et $t_3$. L'estimateur que l'on consid√®re est donc une approximation de $\tilde \theta_{13}$, et est d√©fini par :

$$\hat \theta_{13} = \frac 1 N \sum\limits_{n=1}^N \left| \hat X(t_3) - \hat X(t_1) \right|^2$$

o√π $\hat X$ est la courbe lisss√©e √† partir des observations $( T_i^{[n]}, Y_i^{[n]} )_{n \in 1:N, i \in 1:M_n}$

\input{content/chapter_2/estimation_regularite_locale.tex}

\subsection{Pr√©lissage}


Afin de pouvoir estimer la r√©gularit√© locale des trajectoires, nous allons lisser les trajectoires. En effet, celles-ci sont souvent bruit√©es, et il est n√©cessaire de lisser les trajectoires afin de pouvoir estimer la r√©gularit√© locale de fa√ßon pertinente. De plus si on veut estimer la r√©gularit√© en un point non observ√©, il devient alors n√©cessaire de lisser les trajectoires afin de pouvoir estimer la r√©gularit√© en ce point. Cette √©tape de lissage est appel√©e pr√©lissage de la courbe.

\question{
    \smallskip\centering
    Pourquoi parle-t-on de \textbf{pr√©}-lissage ? Le but de consid√©rer la r√©gularit√© n'√©tait-il pas justement de l'utiliser dans le lissage des trajectoires ? Lisser avant m√™me d'estimer la r√©gularit√© n'est-il pas contre-productif ?
}

L'objectif de l'obtention des param√®tres de r√©gularit√© des trajectoires est de pouvoir effectuer un lissage de ces trajectoires qui pr√©serve les irr√©gularit√©s fondamentales du processus dont elles sont issues, tout en √©liminant le bruit. Les param√®tres de r√©gularit√© sont donc dans un premier temps estim√©s en utilisant des trajectoires liss√©es puis utilis√©s pour effectuer un nouveau lissage √† noyaux en utilisant une fen√™tre de lissage appropri√©e qui d√©pend de ces param√®tres. 
% @ todo : expression de h_ùõº(t)
\editorwarn{inclure l'expression de $h_\alpha(t)$}
% @ ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî


\subsubsection{pr√©-lissage Spline}

Le lissage spline est certainement une des m√©thodes de lissage les plus r√©pandues de par sa simplicit√© d'impl√©mentation. De plus la d√©termination des hyper-param√®tres de lissage via la m√©thode de GCV permet de d√©terminer une approximation de base optimale √† un co√ªt computationnel relativement faible. Un des plus grands avantages du lissage B-Spline est l'obtention d'une base de fonctions, qui permet √† co√ªt de stockage faible de pouvoir pr√©dire des points non observ√©s. Une fois la base d√©termin√©e, il ne reste plus qu'√† pr√©dire les points non observ√©s en utilisant la base de fonctions et les coefficients de la d√©composition de la courbe sur cette base.

\bigskip



\subsubsection{pr√©-lissage √† noyaux}



\subsubsection{√©crasement des irr√©gularit√©s ? vers le pr√©-lissage √† ondelettes}

Le lissage spline donne une fonction de classe $\mathcal C^2$, ce qui est un d√©savantage dans le cadre du pr√©lissage qui sert √† d√©terminer les param√®tres de r√©gularit√© de courbes issues d'un processus que l'on ne suppose pas plus r√©gulier que continu. Toutefois, le fait d'utiliser une base de fonctions pour effectuer le lissage a de nombreux avantages par rapport au lissage √† noyaux qui peuvent √©ventuellement s'av√©rer utiles dans certaines situations sp√©cifiques pour la mise en production de mod√®les.

En effet, une fois que l'on a d√©termin√© les composantes de la d√©composition de notre signal sur la base de fonctions, on n'a plus besoin de se r√©f√©rer aux donn√©es pour pr√©dire une valeur. Il s'agit d'une m√©thode tr√®s √©conome en m√©moire, ce qui peut √™tre tr√®s avantageux dans le cadre de la mise en production de mod√®les lorsqu'il y a de nombreuses courbes observ√©es.


\input{content/chapter_2/prelissage_wavelet.tex}


\section{Estimation adaptative}

\subsection{Estimation adaptative de la fonction moyenne}

\input{content/chapter_2/estimation_adaptative__moyenne.tex}


\subsection{Estimation adaptative de l'op√©rateur de covariance}

\input{content/chapter_2/estimation_adaptative__covariance.tex}

\subsection{Estimation adaptative de l'auto-covariance des s√©ries temporelles fonctionnelles}

\input{content/chapter_2/estimation_adaptative__autocovariance.tex}