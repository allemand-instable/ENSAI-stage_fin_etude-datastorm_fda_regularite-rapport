\chapter{M√©thodologie}
\minitoc%

\section{Donn√©es Fonctionnelles : l'essentiel}

    \input{content/chapter_2/fda.tex}

    \subsection{Cas non ind√©pendant : s√©ries temporelles de donn√©es fonctionnelles}
        
    \input{content/chapter_2/fda_ts.tex}

\pagebreak

\section{Estimation de la r√©gularit√© locale des trajectoires}

\subsection{Ce qu'on entend par r√©gularit√© locale}

\input{content/chapter_2/regularite_locale.tex}


\subsection{Estimation des param√®tres r√©gularit√© locale des trajectoire}

\subsubsection{Deux m√©thodes d'obtention de la r√©gularit√© locale des trajectoires}

Il existe deux m√©thodes diff√©rentes pour estimer la r√©gularit√© des trajectoires. Si la cl√© des deux m√©thodes pour extraire la r√©gularit√© locale est le th√©or√®me de continuit√© de Kolmogorov √©nonc√© ci-dessous, les deux m√©thodes diff√®rent par les points $t \in \mathcal T$ consid√©r√©s dans l'estimation des accroissements quadratiques $\esperance{ \vert X(u) - X(v) \vert^2 }$ utilis√©s pour l'estimation de la r√©gularit√© locale. 

\begin{thm}[Continuit√© de Kolmogorov]
    \emph{r√©f√©rence : } ~\cite[thm : 2.197 | page : 145]{capasso2015introduction}

    $$
    \begin{array}{ll}
        \textsf{\faCaretSquareRight} 
        & X : \, \begin{array}{ccc}
            \mathbb{R_+} \times \Omega & \longrightarrow & \mathbb{R} \\
            (t, \omega) & \longmapsto & X(t, \omega) = x(t)
            \end{array} \textsf{s√©parable}
        & &
        \\
        \textsf{\faCaretSquareRight}
        & \exists r,c, \varepsilon, \delta \in \mathds R_+ \quad (\forall h < \delta)(\forall t \in \mathds R_+)  \quad \esperance{ | X(t+h) - X(t) |^r } \leq c\cdot h^{1+\varepsilon}
    \end{array}
    $$
    \begin{center}
        $\Downarrow$
    \end{center}
    $$ 
    \textbf{\faIcon{asterisk}}\, \boxed{
        X \textsf{ est continu en } t \in \mathds R_+ \textsf{ pour presque tout } \omega \in \Omega
    }
    $$
    \begin{center}
        ie : il existe une version $\tilde X$ de $X$ continue en $t$ telle que $\proba{ \tilde X(t) = X(t)} = 1$
    \end{center}

    $$
    \textbf{\faIcon{asterisk}} \, \boxed{
        \tilde{X} \textsf{ est } \gamma \textsf{-H√∂lderienne en } t  \textsf{ pour tout } 0 < \gamma < \frac{\varepsilon}{r}
    }
    $$
    \label{thm:kolmogorov_continuite}
\end{thm}

Etant donn√© que notre estimateur utilise les incr√©ments quadratiques, on se place dans le cas o√π $r = 2$.

% \editorwarn{Mettre la version avec H√∂lder, qui permet de d√©river des fda la r√©gularit√© locale}


La m√©thode de Golovkine et al. ~\cite[pages : 7‚Äî9]{golovkineRegularityOnlineEstimationNoisyCurve} n'utilise que les points observ√©s, et construit un estimateur des incr√©ments quadratiques √† base de statistique d'ordre. 

$$\theta( T_{(l)}, T_{(k)}) = \esperance{ \left| X( T_{(l)}) - X(T_{(k)}) \right|^2 }  \begin{align*}
    &\quad \underset {\textsf{LGN}} \approx 
    & \boxed{\frac 1 {N} \sum\limits_{n=1}^N \left| \statrang Y n {2k-1} \statrang Y n k \right|^2 \isdef \hat \theta_k}
    \\
    & \, \underset {+ \mathcal C^0 Kol.} {\overset {\textsf{H√∂lder}} \approx}
    & L_{t_0} \esperance{| \ordered T l - \ordered T k |^{2H_{t_0}}} 
    & \rightarrow H_{t_0} = f(\theta)
\end{align*}  $$

$$\hat H_{t_0}(k) = \begin{cases} \displaystyle\frac{\log\left( \hat \theta_{4k-3} - \hat \theta_{2k-1}  \right) - \log \left(  \hat\theta_{2k-1} - \hat \theta_k \right)}{2\log 2} & \hat \theta_{4k-3} > \hat \theta_{2k-1} > \hat \theta_{k}
    \\
    1 & \textsf{sinon}
\end{cases}$$

\info{Cette m√©thode peut s'av√©rer sp√©cifiquement utile lorsque l'on traite un flux de donn√©es, car l'arriv√©e de nouvelles donn√©es ne n√©cessite pas sp√©cifiquement de recalculer les incr√©ments quadratiques sur l'ensemble des points observ√©s. }

L'autre m√©thode propos√©e par ~\cite{golovkine2021adaptive,maissoro-SmoothnessFTSweakDep}, elle se base sur l'utilisation de points non observ√©s, inf√©r√©s par lissage des courbes, √† une distance $\Delta / 2$ les uns des autres pour estimer les incr√©ments quadratiques. Cette derni√®re m√©thode implique le choix d'un hyper-param√®tre lors de l'estimation $\Delta$ et pourrait √™tre sensible √† la qualit√© du lissage de la courbe. Etant donn√© que l'objectif de la d√©termination de la r√©gularit√© locale est de pouvoir faire un lissage √† noyaux adaptatif en fonction de l'objet que l'on souhaite estimer, on appelle le lissage effectu√© pour estimer la r√©gularit√© \og pr√©-lissage \fg.

% https://tex.stackexchange.com/questions/156993/plotting-weierstrass-function
\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\linewidth}
        \input{content/chapter_2/plot_weier_delta_1.tex}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\linewidth}
        \input{content/chapter_2/plot_weier_delta_2.tex}
    \end{minipage}
    \caption{Illustration de la m√©thode \og pr√©lissage \fg pour estimer la r√©gularit√© locale.}
    \label{fig:delta_method_example}
\end{figure}
\smallskip

On se donne un $\Delta \in \, ] \, 0,1 \,[$, arbitraire pour le moment, comme diam√®tre de l'intervalle $J_\Delta$ que l'on consid√®re pour √©valuer la r√©gularit√© en $t_0$.

Il est naturel de d√©finir les points d'estimation de la r√©gularit√© de la fa√ßon suivante :

$$t_1 \isdef t_0 - \frac \Delta 2$$

$$t_2 \isdef t_0$$ 

$$t_3 \isdef t_0 + \frac \Delta 2$$

avec $t_0$ le point en lequel on souhaite estimer la r√©gularit√©.

\info{
    \begin{rem}
        
        Rien n'emp√™che dans la th√©orie d'avoir les points $t_1, t_2, t3$ non ordonn√©s dans le temps, mais dans la pratique, on consid√®re naturellement que $t_1 < t_2 < t_3$. 
        
        Seule la condition $t_1, t_2, t_3 \in J_\Delta$ importe pour l'estimation de la r√©gularit√© locale. \editorwarn{v√©rifier si il faut basolument √™tre √©quidistant} Ainsi aux bords, si l'on souhaite estimer la r√©gularit√© au point $t_0$ tel que la d√©finition pr√©c√©dente nous donne un point $t_1$ en dehors de $[0,1]$, on peut tout √† fait √† la place consid√©rer :

            \begin{minipage}{0.5\linewidth}
                $$t_2 \isdef t_0$$
                $$t_1 \isdef t_0 + \frac \Delta 2$$
                $$t_3 \isdef t_0 + \Delta$$
            \end{minipage}
            \begin{minipage}{0.5\linewidth}
                \centering
                on pourra se r√©f√©rer √† la 2$^e$ image de la figure \ref{fig:delta_method_example}
            \end{minipage}

        
        
    \end{rem}
}

alors on approche $\theta (t_1,t_3) = \esperance{ \left| X(t_3) - X(t_1) \right|^2 } = \theta_{13}$ par :

$$\tilde \theta_{13} = \frac 1 N \sum\limits_{n=1}^N \left| X(t_3) - X(t_1) |^2$$

qui n'est pas observable, √©tant donn√© qu'il n'est pas garanti d'observer $X(t_1)$ et $X(t_3)$, et qu'il faut donc lisser dans un premier temps les courbes pour pouvoir √©valuer $X$ en $t_1$ et $t_3$. L'estimateur que l'on consid√®re est donc une approximation de $\tilde \theta_{13}$, et est d√©fini par :

$$\hat \theta_{13} = \frac 1 N \sum\limits_{n=1}^N \left| \hat X(t_3) - \hat X(t_1) \right|^2$$

o√π $\hat X$ est la courbe lisss√©e √† partir des observations $( T_i^{[n]}, Y_i^{[n]} )_{n \in 1:N, i \in 1:M_n}$

\input{content/chapter_2/estimation_regularite_locale.tex}

\subsection{Pr√©lissage}


Afin de pouvoir estimer la r√©gularit√© locale des trajectoires, nous allons lisser les trajectoires. En effet, celles-ci sont souvent bruit√©es, et il est n√©cessaire de lisser les trajectoires afin de pouvoir estimer la r√©gularit√© locale de fa√ßon pertinente. De plus si on veut estimer la r√©gularit√© en un point non observ√©, il devient alors n√©cessaire de lisser les trajectoires afin de pouvoir estimer la r√©gularit√© en ce point. Cette √©tape de lissage est appel√©e pr√©lissage de la courbe.

\question{
    \smallskip\centering
    Pourquoi parle-t-on de \textbf{pr√©}-lissage ? Le but de consid√©rer la r√©gularit√© n'√©tait-il pas justement de l'utiliser dans le lissage des trajectoires ? Lisser avant m√™me d'estimer la r√©gularit√© n'est-il pas contre-productif ?
}

L'objectif de l'obtention des param√®tres de r√©gularit√© des trajectoires est de pouvoir effectuer un lissage de ces trajectoires qui pr√©serve les irr√©gularit√©s fondamentales du processus dont elles sont issues, tout en √©liminant le bruit. Les param√®tres de r√©gularit√© sont donc dans un premier temps estim√©s en utilisant des trajectoires liss√©es puis utilis√©s pour effectuer un nouveau lissage √† noyaux en utilisant une fen√™tre de lissage appropri√©e qui d√©pend de ces param√®tres. 
% @ todo : expression de h_ùõº(t)
\editorwarn{inclure l'expression de $h_\alpha(t)$}
% @ ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

\bigskip

Le coeur de ce stage est la d√©termination du comportement d√© l'hyper-param√®tre $\Delta$, diam√®tre de l'intervalle que l'on consid√®re dans lequel on vient prendre la valeur de notre processus en 3 points r√©guli√®rement espac√©s (\editorwarn{v√©rifier que c'est obligatoire}). \cite{maissoro-SmoothnessFTSweakDep} affirme d√©j√† que pour un $\Delta$ donn√©, on a bien la convergence ponctuelle des estimateurs. Ces points ne sont pas n√©cessairement observ√©s, et on va donc effectuer un pr√©-lissage. 

\smallskip

Toutefois, le praticien est en droit de se demander quel $\Delta$ explicitement choisir ? Est ce qu'il y a une proc√©dure simple pour d√©terminer la valeur optimale de $\Delta$ qu'il faut choisir pour obtenir un biais le plus petit possible pour l'estimation des param√®tres de r√©gularit√© ?

\question{ la m√©thode de pr√©-lissage a-t-elle une importance ? SI oui, laquelle faut-il choisir ?}

C'est ce que nous allons voir dans un premier temps avant d'aborder le comportement du $\Delta$

\subsubsection{pr√©-lissage Spline}

Le lissage spline est certainement une des m√©thodes de lissage les plus r√©pandues de par sa simplicit√© d'impl√©mentation. De plus la d√©termination des hyper-param√®tres de lissage via la m√©thode de GCV permet de d√©terminer une approximation de base optimale √† un co√ªt computationnel relativement faible. Un des plus grands avantages du lissage B-Spline est l'obtention d'une base de fonctions, qui permet √† co√ªt de stockage faible de pouvoir pr√©dire des points non observ√©s. Une fois la base d√©termin√©e, il ne reste plus qu'√† pr√©dire les points non observ√©s en utilisant la base de fonctions et les coefficients de la d√©composition de la courbe sur cette base.



\subsubsection{pr√©-lissage √† noyaux}





\subsubsection{Lisser en utilisant une base de fonction sans √©craser l'information irr√©guli√®re ?}

Le lissage spline donne une fonction de classe $\mathcal C^2$, ce qui est un d√©savantage dans le cadre du pr√©lissage qui sert √† d√©terminer les param√®tres de r√©gularit√© de courbes issues d'un processus que l'on ne suppose pas plus r√©gulier que continu. Toutefois, le fait d'utiliser une base de fonctions pour effectuer le lissage a de nombreux avantages par rapport au lissage √† noyaux qui peuvent √©ventuellement s'av√©rer utiles dans certaines situations sp√©cifiques pour la mise en production de mod√®les.

En effet, une fois que l'on a d√©termin√© les composantes de la d√©composition de notre signal sur la base de fonctions, on n'a plus besoin de se r√©f√©rer aux donn√©es pour pr√©dire une valeur. Il s'agit d'une m√©thode tr√®s √©conome en m√©moire, ce qui peut √™tre tr√®s avantageux dans le cadre de la mise en production de mod√®les lorsqu'il y a de nombreuses courbes observ√©es.


\input{content/chapter_2/prelissage_wavelet.tex}


\section{Estimation adaptative}

\subsection{Estimation adaptative de la fonction moyenne}

\input{content/chapter_2/estimation_adaptative__moyenne.tex}


\subsection{Estimation adaptative de l'op√©rateur de covariance}

\input{content/chapter_2/estimation_adaptative__covariance.tex}

\subsection{Estimation adaptative de l'auto-covariance des s√©ries temporelles fonctionnelles}

\input{content/chapter_2/estimation_adaptative__autocovariance.tex}