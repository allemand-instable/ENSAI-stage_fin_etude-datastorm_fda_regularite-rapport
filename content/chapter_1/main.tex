\chapter{Motivations}
\minitoc%

\input{content/chapter_1/les_donnees.tex}

\smallskip

\input{content/chapter_1/arima.tex}

\bigskip

\noindent
\fbox{%
  \parbox{\textwidth}{
    Afin de prédire sur le long terme, nous allons donc adopter une approche basée sur les données fonctionnelles pour capturer la structure de la consommation. Cette approche permettra de d'exploiter une information clé : la similarité entre les courbes observées. 
    }%
}

\input{content/chapter_1/explication_fda.tex}

\pagebreak

\input{content/chapter_1/histoire_fda.tex}

\pagebreak


Maintenant que l'on possède une meilleure intuition de ce que sont les données fonctionnelles, il est naturel de se demander pourquoi le choix de modéliser notre phénomène par des données fonctionnelles serait particulièrement judicieux. Pour cela, rappelons nous les difficultés que l'on avait rencontrées dans le cadre de nos données de production électrique en utilisant un modèle de série temporelle classique :

\info{
    \textbf{Rappel : }

    "Ainsi le modèle \colorize{(arima)} sélectionné considérait les irrégularités comme étant du bruit $[\ldots]$ Afin de prédire sur le long terme, nous allons donc adopter une approche basée sur les données fonctionnelles pour capturer la structure de la consommation $[\ldots]$"
}


\question{\smallskip
\centering
    Pourquoi est-ce que l'on s'intéresse autant à la régularité des données que l'on étudie ici ? Et surtout, en quoi est ce que les données fonctionnelles vont nous permettre de mieux capturer la régularité ?
    }

Comme mentionné auparavant, la production électrique est un phénomène très irrégulier [~\ref{fig:courbes_de_charge}] étant influencé par la consommation, la météo, etc. Par conséquent, la prévision de ces courbes de charge doit prendre en compte la nature fondamentalement irrégulière du phénomène afin de proprement le modéliser et, en définitive, mieux le prédire. Ce qui est notamment contraire à la plupart des méthodes qui utilisent des fonctions de classe $\mathcal C^2$ pour lisser les points observés en données fonctionnelles, ce qui limite la prédiction à des courbes de nature $\mathcal C^2$. Cela est d'autant plus critique lorsque l'on cherche à estimer le processus moyen ou l'opérateur de covariance du processus, car ces derniers sont estimés à partir des courbes lissées, qui détruisent toute l'information irrégulière si elle n'est pas prise en compte, impactant significativement l'estimation des objets qui nous intéressent en tant que statisticien.

\editlater{introduire photo qui superpose un mouvement brownien et le lissage spline de ce même mouvement brownien}

Il est ainsi important pour des phénomènes de nature irrégulière de ne pas négliger des précautions lors du lissage afin de ne pas perdre l'information irrégulière. L'idée est donc d'estimer dans un premier temps la régularité de notre processus afin de lisser nos données de manière adaptée pour débruiter et prédire des valeurs non observées tout en préservant les informations irrégulières critiques pour la bonne estimation du processus moyen et de l'opérateur de covariance. L'approche fonctionnelle est clé dans l'estimation de cette régularité, car c'est la \textbf{réplication de courbes} de même nature qui permet in-fine d'\textbf{estimer la régularité} du phénomène.

% ~ ——————————————————————————————————————————————————————————






