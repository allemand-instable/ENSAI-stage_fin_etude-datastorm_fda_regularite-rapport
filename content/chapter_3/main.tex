\chapter{Détermination du diamètre optimal des intervalles à considérer pour l'estimation de la régularité locale }
\minitoc%


Nous avons désormais établi que la génération d'un $\operatorname{FAR}(1)$ basé sur un mouvement brownien multi-fractionnaire permettait de contrôler de bout-en-bout la régularité du processus. Ceci va nous permettre de pouvoir analyser correctement le comportement du risque d'estimation de la régularité en fonction de $\Delta$, ainsi que le comportement du $\Delta^*$ optimal.

\section{Choix des paramètres de la simulation des $\operatorname{FAR}(1)$ localement Hölderiennes}

\input{content/chapter_3/02-simulation.tex}

\section{Prélissage des données simulées}

\input{content/chapter_3/03-pre_lissage.tex}


% \section{Qualité de l'estimation des incréments quadratiques moyens}
% \input{content/chapter_3/04-theta_risk.tex}
% 
% \section{Qualité de l'estimation de la régularité locale}
% \input{content/chapter_3/05-H_risk.tex}
\section{Détermination du $\Delta$ optimal à choisir pour l'estimation de la régularité}

\noindent L'étude des courbes de risques obtenues :

\begin{equation*}
\widehat{\mathcal R} \bigl( \, \Theta \, , \, \Delta \, \bigr) 
= 
\frac 1 {mc} \sum\limits_{p=1}^{mc} \frac{{\distnorme 2 {\widehat \Theta\bigl[\, p \,\bigr]} {\widetilde \Theta \bigl[ \,p \,\bigr]}}^2}{{\norme 2 {\widetilde \Theta \bigl[ \,p \,\bigr]}}^2}
\end{equation*}

indiquent la sélection du $\Delta$ par la procédure suivante\footnote{Des détails sur la détermination de la procédure de sélection du $\Delta$ en annexe \ref{annexe:choix_risque_couple}} :

\begin{itemize}
	\item \textbf{Détermination de la fenêtre de pré-lissage :}
	\begin{itemize}

		\item Il est important de prendre en compte les \og trous \fg lors du lissage à noyau des courbes. Il convient donc de ne pas sélectionner les fenêtres de lissage où le lissage à noyau a échoué sur une partie du support. On pourra se référer à l'annexe \ref{annexe:lissage_fail} pour plus de détails.

		\item Pour les données \textbf{sparses} en nombre moyen d'observations par courbe ($\widehat \lambda \leq  100-150$) : Bien lisser les courbes individuellement en déterminant la fenêtre de lissage par validation croisée.
	\end{itemize}


	\bigskip

	\item \textbf{Choix du couple :}

	\begin{itemize}
		\item \textbf{on dispose d'une information a priori sur une idée des zones plus ou moins régulières de données :} Il est conseillé d'estimer le couple dont la deuxième composante (c'est à dire soit $\theta_{12}$ soit $\theta_{23}$) est celle qui pointe vers l'information la plus régulière. Même si les risques entre les couples $\thetaA$ et $\thetaB$ étaient proches, celui qui utilisait $\theta_{23}$ était souvent meilleur. Et-ce, que ce soit pour le risque relatif ou le risque absolu d'ailleurs. Dans le cadre de notre simulation $\theta_{23}$ correspondait à l'information plus régulière (car $t \mapsto H_t$ était croissante).
		\item \textbf{On ne dispose d'aucune information à priori sur la régularité :} Les deux couples ne disposent pas d'un risque qui diffère grandement ($cf$ graphiques \ref{fig:sparse_osef_rel})
	\end{itemize}

\bigskip

\item \textbf{Choix du $\Delta$ : } En se référant aux graphiques en annexe \ref{fig:sparse_osef_rel}, on recommande l'utilisation d'un $\Delta$ relativement grand vis-à-vis du support. Les graphes des risques indiquent que les $\Delta$ de la taille de 10 à 20\% du support étaient à risques relatifs équivalents, et ce pour les différents niveaux de régularité (0.5 à 0.7). C'est pourquoi dans l'optique d'une estimation de la régularité \og locale \fg on recommande plutôt de se situer aux environs de 15-20\% du support pour bénéficier du plateau autour, tout en restant local, étant donnée que les données simulées avaient une régularité qui ne variait pas brusquement sur le support.

\bigskip

\item \textbf{Si l'on tient au risque euclidien (non relatif) : } Il est recommandé pour le risque euclidien non relatif après avoir analysé les graphes \ref{fig:compare_xtrm_2} en ayant retiré les points extrêmes, de prendre un $\Delta$ proche de $0.01$ pour les zones moins régulières ($H_t < 0.6$) et $\Delta$ proche de $0.2$ pour les zones plus régulières ($H_t \geq 0.6$). Si l'on ne possède aucune information, il est possible d'effectuer un premier lissage qui estime la régularité avec $\Delta = 0.01$\footnote{Lorsque $\Delta = 0.2$ est optimal, pour le risque euclidien non relatif à la norme de la cible, $\Delta = 0.01$ n'engendre pas —dans le cadre de la simulation— la pire valeur de risque possible, là où 0.2 donne le pire risque lorsque $0.01$ est optimal. Il est donc plus prudent d'effectuer une première estimation avec $\Delta = 0.01$.}, puis avec cette estimation ré-effectuer un lissage en évaluant cette fois la régularité avec le $\Delta$ qui aura été recommandé dans cette section pour le risque euclidien (non relatif).
\end{itemize}

\section{Discussion}

Il est à noter que le choix du risque apportait une conclusion différente sur la détermination de $\Delta$ à choisir en pratique. Le choix du risque euclidien relatif à la norme de la cible semblait être un choix qui fait plus sens à la fois d'un point de vue méthodologique, mais aussi par chance, un choix qui mène à une règle de détermination assez simple du $\Delta$ avec une courbe de risque qui semble mieux se comporter. 

\bigskip

On pourra noter aussi les différents pics présents à certaines valeurs de $\Delta$ dans les graphes de risque. Après investigation des courbes des réplications de monte-carlo présentant des cas extrêmes ( comme par exemple un risque euclidien non relatif s'élevant jusqu'à 80 — comme sur la figure \ref{fig:dist_R_eucl_curves} ), Il semblerait que cela soit dû à l'estimation de $X(t_1), X(t_3)$ non loin, ou bien même dans un \og trou d'observation \fg. Le praticien doit donc faire attention lors de l'estimation de la régularité si il obtient des résultats peu concluants, si les courbes initiales possèdent peu de points (ou pas de points) proches des temps $t_1(\Delta), t_2(\Delta)$ ou $t_3(\Delta)$. Auquel cas, pour estimer la régularité deux pistes se présentent :

\begin{itemize}
\item estimer la régularité par les statistiques d'ordre, pour le voisinage des points problématiques via la méthode de Golovkine et al.

\bigskip

\item Investiguer sur le potentiel de lisser via une base de fonction, qui ne détruit pas trop les informations de régularité, comme la base d'ondelettes. Ce qui rend la base d'ondelettes particulièrement intéressante est explicité en annexe \ref{annexe:wavelet}.
\end{itemize}
