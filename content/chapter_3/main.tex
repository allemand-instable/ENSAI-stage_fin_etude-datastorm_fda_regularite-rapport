\chapter{Détermination du diamètre optimal des intervalles à considérer pour l'estimation de la régularité locale }
\minitoc%


Nous avons désormais établi que la génération d'un $\operatorname{FAR}(1)$ basé sur un mouvement brownien multi-fractionnaire permettait de contrôler de bout-en-bout la régularité du processus. Ceci va nous permettre de pouvoir analyser correctement le comportement du risque d'estimation de la régularité en fonction de $\Delta$, ainsi que le comportement du $\Delta^*$ optimal.

\section{Choix des paramètres de la simulation des $\operatorname{FAR}(1)$ localement Hölderiennes}

\input{content/chapter_3/02-simulation.tex}

\section{Prélissage des données simulées}

\input{content/chapter_3/03-pre_lissage.tex}


% \section{Qualité de l'estimation des incréments quadratiques moyens}
% \input{content/chapter_3/04-theta_risk.tex}
% 
% \section{Qualité de l'estimation de la régularité locale}
% \input{content/chapter_3/05-H_risk.tex}
\section{Détermination du $\Delta$ optimal à choisir pour l'estimation de la régularité}
\label{sec:determination-delta}

\noindent L'étude des courbes de risques obtenues :

\begin{equation}
\widehat{\mathcal R} \bigl( \, \Theta \, , \, \Delta \, \bigr) 
= 
\frac 1 {mc} \sum\limits_{p=1}^{mc} \frac{{\distnorme 2 {\widehat \Theta\bigl[\, p \,\bigr]} {\widetilde \Theta \bigl[ \,p \,\bigr]}}^2}{{\norme 2 {\widetilde \Theta \bigl[ \,p \,\bigr]}}^2}
\end{equation}

\smallskip
où $\bigl[\, p \,\bigr]$ signifie que la quantité a été calculée à partir de la $p^{eme}$ réplication de monte carlo de la simulation.

\bigskip

indiquent la sélection du $\Delta$ par la procédure suivante\footnote{Des détails sur la détermination de la procédure de sélection du $\Delta$ en annexe \ref{annexe:choix_risque_couple}} :

\begin{itemize}
	\item \textbf{Détermination de la fenêtre de pré-lissage :}
	\begin{itemize}
		
		\item calculer $\widehat \lambda$, le nombre moyen de points par courbe et effectuer une validation croisée sur une grille d'échelle comprise entre $\displaystyle\frac{1}{\widehat \lambda}$ et $\displaystyle\frac{1}{\widehat \lambda^3}$

		\item Il est important de prendre en compte les \og trous \fg lors du lissage à noyau des courbes. Il convient donc de ne pas sélectionner les fenêtres de lissage où le lissage à noyau a échoué sur une partie du support.\footnote{On pourra se référer à l'annexe \ref{annexe:lissage_fail} pour plus de détails.}

		\item Pour les données \textbf{\og sparses \fg} en nombre moyen d'observations par courbe ($\widehat \lambda \leq  100-150$) : Bien lisser les courbes individuellement en déterminant la fenêtre de lissage par validation croisée. Pour les données \textbf{\og denses \fg} en nombre moyen d'observations par courbes, économiser du temps de calcul est possible en effectuant un lissage global médian sur les fenêtres cross-validées des premières courbes comme indiquent les tables \ref{tab:couple_1312_indiv_vs_glob} et \ref{tab:stat_R_eucl_min_max_q}.
	\end{itemize}


	\bigskip

	\item \textbf{Choix du couple :}

	\begin{itemize}
		\item \textbf{on dispose d'une information a priori sur une idée des zones plus ou moins régulières de données :} Il est conseillé d'estimer le couple dont la deuxième composante (c'est à dire soit $\theta_{12}$ soit $\theta_{23}$) est celle qui pointe vers l'information la plus régulière. Même si les risques entre les couples $\thetaA$ et $\thetaB$ étaient proches, celui qui utilisait $\theta_{23}$ était souvent meilleur. Et-ce, que ce soit pour le risque relatif ou le risque absolu d'ailleurs. Dans le cadre de notre simulation $\theta_{23}$ correspondait à l'information plus régulière (car $t \mapsto H_t$ était croissante).
		\item \textbf{On ne dispose d'aucune information à priori sur la régularité :} Les deux couples ne disposent pas d'un risque qui diffère grandement ($cf$ graphiques \ref{fig:sparse_osef_rel})
	\end{itemize}

\bigskip

\item \textbf{Choix du $\Delta$ : } En se référant aux graphiques en annexe \ref{fig:sparse_osef_rel}, on recommande l'utilisation d'un $\Delta$ relativement grand vis-à-vis du support. Les graphes des risques indiquent que les $\Delta$ de la taille de 10 à 20\% du support étaient à risques relatifs équivalents, et ce pour les différents niveaux de régularité (0.5 à 0.7). C'est pourquoi dans l'optique d'une estimation de la régularité \og locale \fg on recommande plutôt de se situer aux environs de 10-15\% du support pour bénéficier du plateau autour, tout en restant \og local \fg.\footnote{sera détaillé en discussion}

\bigskip

\item \textbf{Si l'on tient au risque euclidien (non relatif) : } Il est recommandé pour le risque euclidien non relatif après avoir analysé les graphes \ref{fig:compare_xtrm_2} en ayant retiré les points extrêmes, de prendre un $\Delta$ proche de $0.01$ pour les zones moins régulières ($H_t < 0.6$) et $\Delta$ proche de $0.2$ pour les zones plus régulières ($H_t \geq 0.6$). Si l'on ne possède aucune information, il est possible d'effectuer un premier lissage qui estime la régularité avec $\Delta = 0.01$, puis avec cette estimation ré-effectuer un lissage en évaluant cette fois la régularité avec le $\Delta$ qui aura été recommandé dans cette section pour le risque euclidien (non relatif).\footnote{Lorsque $\Delta = 0.2$ est optimal, pour le risque euclidien non relatif à la norme de la cible, $\Delta = 0.01$ n'engendre pas —dans le cadre de la simulation— la pire valeur de risque possible, là où 0.2 donne le pire risque lorsque $0.01$ est optimal. Il est donc plus prudent d'effectuer une première estimation avec $\Delta = 0.01$.}
\end{itemize}

\section{Discussion}

Il est important de garder en tête le modèle dans lequel on s'est placé pour étudier le comportement du $\Delta$. La recommendation qui est faite pour la sélection du $\Delta$ est valable pour :

\warn{
	\begin{itemize}
		\item $\operatorname{FAR}(1)$ construit à partir d'un $\operatorname{mfBm}(H, L)$
		\item la régularité donnée par $H(t)$, qui dans notre cas : $H \in \mathcal C^\infty\bigl([0,1]\bigr)$
		\item le noyau de la relation auto-régressive $\beta$ est une fonction de classe $\mathcal C^{\infty}$ sur $]0, 1]$ et continue en $0$
		\item La dérivée de $H$ est $H' :t \mapsto \frac{2 e ^{-5(t-0.5)}}{\left(1 + e ^{-5(t-0.5)}\right)^2}$, la variation maximale de la régularité est atteinte en $\argmax\limits_{t \in [0,1]} H'(t) = \frac 1 2$ avec $H'(\frac 1 2)=\frac 1 2$
		\item La régularité est monotone et strictement croissante sur $[0,1]$
		\item la constante \og locale \fg de Hölder $L : t \mapsto L_t$ est en réalité constante sur l'ensemble du support dans le cadre de nos simulations
	\end{itemize}
}

Trop s'éloigner de ces hypothèses pourrait demander d'analyser de nouveau le comportement du $\Delta$ dû aux propriétés de certaines de ces quantités qui auraient pu influencer le résultat. C'est pourquoi on recommande avec des affirmations du type \og vers le point le plus régulier \fg plutôt qu'avec la position.

Un exemple de l'importance de ces hypothèses et du cadre de simulation est que le $\Delta$ optimal obtenu pour le risque euclidien relatif était dans la grande majorité des cas $\Delta^* = 0.2$. Ce qui peut sembler surprenant pour l'estimation d'une régularité \emph{locale}. Toutefois, la fonction de Hurst est très régulière et varie même lentement sur la majorité du support. Ainsi il est plus raisonnable de penser que l'on peut aller chercher de l'information plus loin, sans trop perdre l'information locale. 

De plus, il est à noter que le choix du risque apportait une conclusion différente sur la détermination de $\Delta$ à choisir en pratique. Le choix du risque euclidien relatif à la norme de la cible semblait être un choix qui fait plus sens à la fois d'un point de vue méthodologique (étant donné que la norme de la cible varie avec $\Delta$) mais aussi par chance qui se trouve être un choix qui mène à une règle de détermination assez simple du $\Delta$ avec une courbe de risque qui semble mieux se comporter. 

\bigskip

On pourra enfin noter les différents pics présents à certaines valeurs de $\Delta$ dans les graphes de risque. Après investigation des courbes des réplications de monte-carlo présentant des cas extrêmes ( comme par exemple un risque euclidien non relatif s'élevant jusqu'à 80 — comme sur la figure \ref{fig:dist_R_eucl_curves} ), Il semblerait que cela soit dû à l'estimation de $X(t_1), X(t_3)$ non loin, ou bien même dans un \og trou d'observation \fg. Le praticien doit donc faire attention lors de l'estimation de la régularité si il obtient des résultats peu concluants, si les courbes initiales possèdent peu de points (ou pas de points) proches des temps $t_1(\Delta), t_2(\Delta)$ ou $t_3(\Delta)$. Auquel cas, pour estimer la régularité deux pistes se présentent :

\bigskip

\begin{itemize}
\item estimer la régularité par les statistiques d'ordre, pour le voisinage des points problématiques via la méthode de Golovkine et al. Il s'agit donc de la méthode recommandée pour les cas pathologiques. 

\bigskip

\item \textbf{( \faExclamationTriangle conjecture )} Investiguer sur le potentiel de lisser via une base de fonction, qui ne détruit pas trop les informations de régularité, comme la base d'ondelettes. Ce qui rend la base d'ondelettes particulièrement intéressante est d'ailleurs explicité en annexe \ref{annexe:wavelet}.
\end{itemize}

