\blackboxed{ \faPen : Explication de la correction }
\begin{leftbar}
    La dépendance dite de \og alpha-mixing \fg n'est pas une dépendance \og forte \fg. C'est en fait le contraire : il s'agit d'une dépendance faible mais d'un autre type de dépendance. Il y a plusieurs façon de faire de la dépendance faible : le \og $\alpha$-mixing \fg et la \og $\mathds L^p-a$ approximation \fg. L'une n'est pas spécialement plus faible que l'autre mais chacune peut avoir son avantage dans certains contextes théoriques et bien choisir sa dépendance faible (qui restent tout de même des concepts non équivalents donc faut faire attention et bien revenir à la défnition de celle que l'on utilise) peut alors permettre de rendre la démonstration de convergence des estimateurs qui nous intéressent plus facile. 

    La confusion vient du fait que l'on lit en anglais dans la littérature \og strongly mixing \fg ou \og strongly $\alpha$-mixing \fg. Il existe des cas où les données sont à la fois \og strongly $\alpha$-mixing \fg et \og $\mathds L^p-a$ approximables \fg.

    \noindent Le point de vue \og strongly-mixing \fg est en résumé une autre perspective de la dépendance faible qui regarde le défaut de dépendance au niveau de la mesure de probabilité par la caractérisation (souvent aussi définition chez la grande majorité des auteurs) :

    \begin{equation*}
        A \indep B \iff \proba{ A \bigcap B } = \proba A \cdot \proba B  
    \end{equation*}
\end{leftbar}

Il existe plusieurs façons de définir une \og dépendance faible \fg, notamment la dépendance dite de \og $\alpha$-mixing \fg comme définie dans ~\cite{estimation-dependent-strong-mixing} \edited :


\begin{definition*}[$\alpha-$mixing]

    une suite $X = \suite X i$ de variables aléatoire est dite $\alpha$-mixing si pour tout $n \in \mathds N$


    $$
        \alpha(n) \tend n \infty 0
    $$

    avec : $\alpha(n) = \sup\limits_k \bigl\{ \lvert \proba{A \cap B} - \proba{A}\proba{B} \rvert \quad | \quad A \in \sigma( X_{1:k} ), \, B \in \sigma(X_{k+n : \infty}) \bigr\}$

    en d'autres termes, la \og dépendance \fg \colorize[flatuicolors_blue_devil]{$(\lvert\, \proba{A \cap B} - \proba{A}\proba{B} \,\rvert)$} entre les variables aléatoires $X_k$ et $X_{k+n}$ tend vers 0 lorsque $n$ tend vers l'infini.
\end{definition*}

Dans ce point de vue on manipule directement les tribus engendrées par les différents stades de passé de la série temporelle et on regarde leur degré d'indépendance via la mesure de probabilité (\edited). Il ne s'agit pas de l'approche considérée par MPV (\edited), en se reposant non pas sur l'indépendance des tribus engendrées par le passé de la série temporelle mais en exploitant la qualité d'approximation de la série temporelle que l'on étudie par un autre processus, indépendant de la série temporelle étudiée à partir d'un certain rang. La définition de dépendance temporelle est alors dite \og faible \fg \textbf{car il existe de la dépendance mais qui décroit rapidement} (\edited). Le point de vue \textcolor{flatuicolors_rose}{\sout{{faible}}} \textbf{adopté par MPV} offre un comportement plus sympathique pour l'aspect \emph{local} dans l'estimation de la régularité : qui est le coeur de l'approche de MPV.
