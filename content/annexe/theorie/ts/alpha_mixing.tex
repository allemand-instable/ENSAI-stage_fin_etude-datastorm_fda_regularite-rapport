\blackboxed{ \faPen : Explication de la correction }
\begin{leftbar}
    La dépendance dite de \og alpha-mixing \fg n'est pas une dépendance \og forte \fg. C'est en fait le contraire : il s'agit d'une dépendance faible mais d'un autre type de dépendance. Il y a plusieurs façon de faire de la dépendance faible : le \og $\alpha$-mixing \fg et la \og $\mathds L^p-a$ approximation \fg. L'une n'est pas spécialement plus faible que l'autre mais chacune peut avoir son avantage dans certains contextes théoriques et bien choisir sa dépendance faible (qui restent tout de même des concepts non équivalents donc faut faire attention et bien revenir à la défnition de celle que l'on utilise) peut alors permettre de rendre la démonstration de convergence des estimateurs qui nous intéressent plus facile. 

    La confusion vient du fait que l'on lit en anglais dans la littérature \og strongly mixing \fg ou \og strongly $\alpha$-mixing \fg. Il existe des cas où les données sont à la fois \og strongly $\alpha$-mixing \fg et \og $\mathds L^p-a$ approximables \fg.

    \noindent Le point de vue \og strongly-mixing \fg est en résumé une autre perspective de la dépendance faible qui regarde le défaut de dépendance au niveau de la mesure de probabilité par la caractérisation (souvent aussi définition chez la grande majorité des auteurs) :

    \begin{equation*}
        A \indep B \iff \proba{ A \bigcap B } = \proba A \cdot \proba B  
    \end{equation*}
\end{leftbar}

Il existe plusieurs façons de définir une \og dépendance faible \fg, notamment la dépendance dite de \og $\alpha$-mixing \fg comme définie dans ~\cite{estimation-dependent-strong-mixing} \edited :


\begin{definition*}[$\alpha-$mixing]

    une suite $X = \suite X i$ de variables aléatoire est dite $\alpha$-mixing si pour tout $n \in \mathds N$


    $$
        \alpha(n) \tend n \infty 0
    $$

    avec : $\alpha(n) = \sup\limits_k \bigl\{ \lvert \proba{A \cap B} - \proba{A}\proba{B} \rvert \quad | \quad A \in \sigma( X_{1:k} ), \, B \in \sigma(X_{k+n : \infty}) \bigr\}$

    en d'autres termes, la \og dépendance \fg \colorize[flatuicolors_blue_devil]{$(\lvert\, \proba{A \cap B} - \proba{A}\proba{B} \,\rvert)$} entre les variables aléatoires $X_k$ et $X_{k+n}$ tend vers 0 lorsque $n$ tend vers l'infini.
\end{definition*}

Dans ce point de vue on manipule directement les tribus engendrées par les différents stades de passé de la série temporelle et on regarde leur degré d'indépendance via la mesure de probabilité (\edited). Il ne s'agit pas de l'approche considérée par MPV (\edited), en se reposant non pas sur l'indépendance des tribus engendrées par le passé de la série temporelle mais en exploitant la qualité d'approximation de la série temporelle que l'on étudie par un autre processus, indépendant de la série temporelle étudiée à partir d'un certain rang. La définition de dépendance temporelle est alors dite \og faible \fg \textbf{car il existe de la dépendance mais qui décroit rapidement} (\edited). Le point de vue \textcolor{flatuicolors_rose}{\sout{{faible}}} \textbf{adopté par MPV} offre un comportement plus sympathique pour l'aspect \emph{local} dans l'estimation de la régularité : qui est le coeur de l'approche de MPV.

Hormann et Kokoszka \cite{10.1214/09-AOS768} parlent des avantages de l'approche de l'approximation $\mathds L^p-a$ par rapport à l'approche du \og strongly-mixing \fg dans leur article \og Weakly dependent functional data \fg :

\citer{

    \blueboxed{\faPen citation rajoutée}

    \bigskip

    L'approche classique de la dépendance faible, dévelopée notamment par Rosenblatt et Ibragimov, utilise la propriété dite de \og fort mélange \fg et de ses variants comme les mélanges $\beta$, $\phi$ , $\rho$ et $\alpha$. L'idée générale est de mesurer al dépendance maximale entre deux évènements respectivement du \og passé \fg $\mathcal F_k^-$ et du \og futur \fg $\mathcal F_{k+m}^+$. L'estompage de la mémoire est décrit par cette dépendance maximale qui tenderait vers $0$ lorsque $m$ tend vers l'infini. par exemple le coefficient de mélange $\alpha$ est donné par :

    \begin{equation*}
        \alpha(m) = \sup_{A \in \mathcal F_k^-, B \in \mathcal F_{k+m}^+} \left| \mathbb P(A \cap B) - \mathbb P(A) \mathbb P(B) \right|
    \end{equation*}

    et alors une séquence est dite $\alpha$-mélangeante si $\alpha(m) \underset{m \to \infty}{\longrightarrow} 0$.

    [...]

    Cette méthode donne des résultats très solides (pour un exposé complet de la théorie classique, voir Bradley ), mais il n'est pas possible de vérifier les conditions de mélange du type susmentionné.( voir Bradley ), mais la vérification des conditions de mélange du type ci-dessus n'est pas  n'est pas facile, alors que la vérification de la $Lp$-$m$-approximabilité est presque comme le montrent nos exemples ci-dessous. Ceci est dû au fait que la condition de $Lp$-$m$-approximabilité utilise directement la spécification du modèle $X_n = f (\varepsilon_n , \varepsilon_{n-1}, ...)$. Un autre problème est que même lorsque le mélange s'applique (par exemple, pour les processus de Markov), il nécessite généralement des conditions de régularité fortes et strictes.

    \flushright{- Siegfried H{\"o}rmann and Piotr Kokoszka \cite{10.1214/09-AOS768}}
}