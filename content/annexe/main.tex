\chapter{Détails techniques et théoriques}
\minitoc%

\section{Données fonctionnelles : formellement}
\label{annexe:fda-formel}
\input{content/annexe/theorie/fda/essentiel.tex}

\section{Régularité Locale}
\label{annexe:regularite-locale}
\input{content/annexe/theorie/regularite_locale-formel.tex}

\section{Dépendance Faible et LGN version faible}
\label{annexe:weak_dep}
\input{content/annexe/theorie/ts/main.tex}

\section{Continuité de Kolmogorov}
\label{annexe:continuite_kolmogorov}
\input{content/annexe/theorie/continuite_kolmogorov.tex}

\section{Estimation adaptative}
\label{annexe:estim_adapt}
\input{content/annexe/theorie/estimation_adaptative/main.tex}

\section{Mouvement Brownien}
\label{annexe:brownien}
\input{content/annexe/theorie/brownien.tex}

\section{Théorie de la base d'ondelettes}
\label{annexe:wavelet}

\info{
	Ceci regroupe les éléments théoriques pour la compréhension de la recommendation d'étude de la base ondelettes pour le prélissage lors de l'estimation de la régularité. Pour plus de détails sur les motivations on pourra se référer à l'annexe \ref{annexe:lissage_base_fcn} ainsi que la discussion méthodologique en section \ref{sec:methodo_discussion}.
}

\input{content/annexe/theorie/wavelet.tex}

% ! ——————————————————————————————————————————— !

\chapter{Plus de détails sur l'étude du Risque}

\minitoc%

\section{Pourquoi viser l'estimation des couples d'incréments plutôt que la régularité}
\label{annexe:choix_risque_couple}
\input{content/annexe/risque/couple_risk.tex}

% \section{Détermination d'un critère de choix du diamètre $\Delta$ des intervalles à considérer pour l'estimation de la régularité locale}
% \input{content/annexe/risque/critere.tex}


\section{Peut-on considérer que tous les $\Delta$ conviennent ?}
\label{annexe:tous_theta_conviennent_borne_norme_theta}

Nous considérons le risque euclidien : $\mathcal R(\Theta, \Delta) = \mathds E \distnorme 2 {\widehat \Theta}{\widetilde \Theta}$. C'est un risque naturel à considérer pour une estimation conjointe de deux paramètres. Evidemment, nous ne disposons pas de la loi de $\distnorme 2 {\widehat \Theta}{\widetilde \Theta}$ c'est pourquoi nous calculons $\widehat {\mathcal R}(\Theta, \Delta) = \mathds E \distnorme 2 {\widehat \Theta}{\widetilde \Theta}$

Nous observons sur les différents graphes des risques de l'ordre de grandeur de $10^{-2}$ ou même de $10^{-3}$. La question que l'on se pose désormais est si il est raisonnable de penser que ne pas choisir le $\Delta^*$ optimal n'est pas si important dans l'estimation du couple $\Theta$.

Ce que nous allons observer est qu'il est tout de même préférable de bien déterminer le $\Delta$

\begin{equation}
	\theta(u,v) = \esperanceloi X { \bigl| X(v) - X(u) \bigr|^2 } \leq L_{J(\Delta)}^2 \bigl| v - u \bigr|^{2 H_{J(\Delta)}}
\end{equation}

sachant que l'on évalue :

\begin{equation}
	\textsf{soit }
	\thetaA = \begin{bmatrix} \theta(t_1, t_3) \\ \theta(t_1, t_2) \end{bmatrix}
\end{equation}
\begin{equation}
	\textsf{soit }
	\thetaB = \begin{bmatrix} \theta(t_1, t_3) \\ \theta(t_2, t_3) \end{bmatrix}
\end{equation}


avec :

\begin{equation}
	\begin{array}{ccc}
		|t_3 - t_1| & = \Delta
		\\
		|t_3 - t_2| & = \frac \Delta 2 & = |t_2 - t_1|
	\end{array}\label{eq:couples_diff_delta_value}
\end{equation}

et donc :

\begin{equation}
	\displaystyle
	\begin{array}{rclr}
		\norme 2 \Theta & =    & \sqrt{\theta_{13}^{\,2} + \theta_{12/23}^{\, 2}}
		\\
		                & \leq & \sqrt{ L_{J(\Delta)}^4 \bigl( \, \Delta^{4 H_{J(\Delta)}} \left[ 1 + \frac 1 2 \right]  \, \bigr) }
		\\
		                & =    & L_{J(\Delta)}^2 \cdot \Delta^{2H_{J(\Delta)}} \cdot \sqrt{\frac 3 2}
	\end{array}
\end{equation}

Et donc :

\begin{equation*}
	\norme 2 \Theta \leq L_{J(\Delta)}^2 \cdot \Delta^{2H_{J(\Delta)}} \cdot \sqrt{\frac 3 2}
\end{equation*}

On se réfère à ces bornes même si l'on étudie plutôt $\widetilde \Theta$ car on peut se ramener asymptotiquement à $\Theta$ par la loi des grands nombres grâce à la dépendance faible :

\begin{align}
	 &  & \norme 2 {\widetilde \Theta} & =                                                                                                                      & \norme 2 {\frac 1 N \sum_{i=1}^{N} \begin{bmatrix} | X_i(t_3) - X_i(t_1) |^2 \\ | X_i(t_3) - X_i(t_2) |^2 \end{bmatrix}} &  &
	\\
	 &  &                              & \overset {\textsf{LGN} + \textsf{dep. faible} + \textsf{norme } \mathcal C^0(\R 2 \backslash\{ 0\}) }{\tend N \infty } & \norme 2 \Theta \leq L_{J(\Delta)}^2 \cdot \Delta^{2H_{J(\Delta)}} \cdot \sqrt{\frac 3 2}                                &  &
\end{align}

\begin{rem}
	On peut remplacer le couple $(t_2, t_3)$ par $(t_1, t_2)$ dans la deuxième composante, l'argument reste valide, comme explicité dans l'équation \ref{eq:couples_diff_delta_value}}
\end{rem}

En utilisant les données de la simulation, $L = 1$, on obtient :

\begin{equation}
	\begin{array}{ccc}
		H_{J(\Delta)} = 0.4  & \implies & \norme 2 \Theta
		\begin{cases}
			\lesssim 3 \cdot 10^{-2} \quad & \Delta = 0.01
			\\
			\lesssim 3\cdot 10^{-1} \quad  & \Delta = 0.2
		\end{cases}
		\\\\
		H_{J(\Delta)} = 0.5  & \implies & \norme 2 \Theta
		\begin{cases}
			\lesssim 1 \cdot 10^{-2} \quad & \Delta = 0.01
			\\
			\lesssim 2\cdot 10^{-1} \quad  & \Delta = 0.2
		\end{cases}
		\\\\
		H_{J(\Delta)} = 0.6  & \implies & \norme 2 \Theta
		\begin{cases}
			\lesssim 5 \cdot 10^{-3} \quad & \Delta = 0.01
			\\
			\lesssim 2\cdot 10^{-1} \quad  & \Delta = 0.2
		\end{cases}
		\\\\
		H_{J(\Delta)} = 0.73 & \implies & \norme 2 \Theta
		\begin{cases}
			\lesssim 1 \cdot 10^{-3} \quad & \Delta = 0.01
			\\
			\lesssim 1\cdot 10^{-1} \quad  & \Delta = 0.2
		\end{cases}
	\end{array}
\end{equation}

Ainsi, la différence de risque entre l'optimum et le pire cas étant de l'odre de $10^{-2}$ dans un cas très sparse comme dans la figure \ref{fig:sparse_osef} et dans un cas raisonnablement dense on observe même des différences de l'ordre de $10^{-3}$ pour le plus régulier.

\begin{table}[H]
	\centering
	\begin{tabularx}{0.7\textwidth}{|cc|X|X|c|}
		\toprule
		\textbf{H} & $\mathbf{\lambda}$ & \textbf{Différence : } $\mathbf{\mathcal R_{max} - \mathcal R_{min}}$ & ordre de gradeur de la borne de $\norme 2 \Theta$ & ${\norme 2 \Theta}^2$ \\
		\midrule
		0.51       & 60                 & 3.3 $\cdot 10^{-2}$                                                   & $\Delta^* \simeq 0.2$ : $10^{-1}$                 & $10^{-2}$             \\
		0.51       & 210                & 1.1 $\cdot 10^{-2}$                                                   & $\Delta^* \simeq 0.2$ : $10^{-1}$                 & $10^{-2}$             \\
		\midrule
		0.6        & 60                 & 4.2 $\cdot 10^{-2}$                                                   & $\Delta^* \simeq 0.01$ : $10^{-3}$                & $10^{-6}$             \\
		0.6        & 210                & 1.2 $\cdot 10^{-2}$                                                   & $\Delta^* \simeq 0.01$ : $10^{-3}$                & $10^{-6}$             \\
		\midrule
		0.73       & 60                 & 1.2 $\cdot 10^{-2}$                                                   & $\Delta^* \simeq 0.01$ : $10^{-3}$                & $10^{-6}$             \\
		0.73       & 210                & 5.4 $\cdot 10^{-3}$                                                   & $\Delta^* \simeq 0.01$ : $10^{-3}$                & $10^{-6}$             \\
		\bottomrule
	\end{tabularx}
	\caption{Ordre de grandeur des différences entre le risque euclidien minimum et maximum pour $\Delta \in [0.01, 0.2]$ et la norme de la cible}
	\label{tab:ordre_grandeur_diff_R_norme}
	\addcontentsline{lot}{table}{\numberline{} Comparaison des ordres de grandeur de la norme de la quantité ciblée $\widetilde \Theta$ et de différence entre le risque optimal et maximal.}
\end{table}

\noindent Étant donné que le risque utilisé est homogène à la norme euclidienne au carré, on ne peut dire, du point de vue du risque euclidien, que l'on peut prendre n'importe quel $\Delta$ dans $[0.01, 0.2]$ sans trop de conséquences.
Ce tableau vient motiver la section suivante sur le choix du risque à considérer pour la détermination d'un $\Delta$ optimal. Si la norme de notre cible varie avec $\Delta$, une idée est de plutôt considérer la qualité de l'estimation, relativement à la norme de la cible.



\section{Choix du risque : absolu ou relatif ?}
\label{annexe:choix-du-rique}

\subsection{Distance Euclidienne}

Afin de quantifier la qualité de l'estimation conjointe du couple de $\theta$, il est raisonnable de considérer la distance euclidienne usuelle pour des vecteurs de $\R 2$

\begin{equation*}
	R^{[\,rel\,]}_{mc}(\Theta, \Delta) = {\distnorme 2 {\widehat \Theta(\Delta)} {\widetilde \Theta(\Delta)}}^2
\end{equation*}

% et on nomme $R\cindexA(\Delta) = R( \thetaA , \, \Delta \, )$ et $R\cindexB(\Delta) = R( \thetaB, \, \Delta \, )$

\subsection{Distance Euclidienne Relative}

On va cependant considérer le risque relatif à la norme de la quantité que l'on cible :

\begin{equation*}
	R^{[\,rel\,]}_{mc}(\Theta, \Delta) =\frac{ {\distnorme 2 {\widehat \Theta(\Delta)} {\widetilde \Theta(\Delta)}}^2}{ {\norme 2 {\widetilde \Theta(\Delta)}}^2 }
\end{equation*}

\question{Pourquoi considérer la distance euclidienne relative à la norme de la cible $\widetilde \Theta$ plutôt que la distance euclidienne classique qui est plus simple ?}

Le risque sert à déterminer la qualité de l'estimation du couple $\widetilde \Theta$ par $\widehat \Theta$ à un $\Delta$ donné. Il faut cependant garder à l'esprit que $\Theta$ est en réalité une fonction de $\Delta$ car la valeur de $t_1, t_2, t_3$ dépendent de $\Delta$. Ainsi \emph{la norme de $\widetilde \Theta$ va varier lorsque l'on fait varier $\Delta$}\footnote{il est possible d'obtenir plus de détails en annexe \ref{annexe:tous_theta_conviennent_borne_norme_theta}}. Les risques obtenus via la norme euclidienne sont des risques qui mesurent une différence absolue, mais alors avoir \emph{un risque plus petit qu'un autre n'a pas le même sens pour différents $\Delta$ en termes de qualité d'approximation}. C'est pourquoi nous considérons le risque relatif dans la détermination du critère du choix du $\Delta$.

On pourra cependant observer la différence entre le risque euclidien et le risque euclidien relatif à la norme de la cible en $\Delta$ sur les figures \ref{fig:sparse_osef} et \ref{fig:sparse_osef_rel}.


\begin{comment}
\section{Récapitulatif de la qualité des estimations pour la stratégie de \og lissage global \fg}
\input{content/annexe/risque/tables_glob.tex}
\end{comment}

\section{Etude de l'impact de la méthode de sélection de la fenêtre de pré-lissage sur le risque d'estimation des couples $\Theta$}


% 

\input{content/annexe/risque/h_glob_indiv/h_glob_vs_indiv_txt.tex}
\input{content/annexe/risque/h_glob_indiv/h_glob_vs_indiv_tables.tex}

\section{Gestion des valeurs extrêmes}

Parmi les différentes réplications de Monte-Carlo simulées dans le cadre de ce stage, l'estimation du couple d'incréments sur certaines d'entre elles a parfois échoué. En effet, il existe des réplications où le risque en certains $\Delta$ explosent : c'est précisément ce que montre la figure \ref{fig:dist_R_eucl_curves}. Ces points aberrants ont dans un premier temps été analysés pour tenter de comprendre dans quel cas de figure l'estimation pouvait être amenée à échouer. La piste retenue est que si l'estimation se fait au voisinage de "trous" (peu de points aux alentours sur l'ensemble des courbes observées), alors l'estimation sera de faible qualité et il vaudra mieux utiliser dans ce voisinage l'estimateur utilisant la statistique d'ordre de Golvkine et al.(2022). 

\smallskip

Afin de mieux comprendre la structure de la courbe de risque, les réplications de Monte-Carlo présentant une valeur de risque extrême ont été identifiés. C'est à dire que si il existe une valeur de $\Delta$ pour laquelle le risque $\widehat{\mathcal R}^{[\,rel\,]}_{mc}(\Delta, \Theta)$ est au dessus du $98^{eme}$ percentile de l'ensemble des risques calculés sur tous les échantillons de Monte-Carlo $\bigl\{ \,\mathcal R^{[\,rel\,]}_{mc}(\Delta, \Theta) : \Delta \in \overrightarrow{\Delta}, mc \in \llbracket 1, 200 \rrbracket \, \bigr\}$. Alors la réplication de Monte-Carlo est simplement retirée pour l'ensemble des analyses, comme si elle n'avait pas été simulée, afin de ne pas compter pour les autres valeurs de risque si elle a été retirée pour une valeur de $\Delta$.

\smallskip

Une limite imposée était de préserver au minimum 80\% des réplications de Monte-Carlo. Il est à noter que le risque absolu était aussi plus sujet à des valeurs de risque aberrantes, on peut notamment voir dans le tableau \ref{tab:qualite_estim_increments_relatif_new_sim} que pour le risque relatif, il y a peu de valeurs extrêmes qui perturbent l'estimation du risque et empêchaient de discerner une structure dans le graphe $\Delta \mapsto \widehat{\mathcal R}( \Delta, \Theta )$. Dans le cadre du risque euclidien \og absolu \fg, bien plus de réplications doivent se voir enlevées pour éviter les nombreux pics visibles sur la figure \ref{fig:compare_xtrm_2} (individuel, avec extrêmes).

\section{Graphes : indiv/global \& valeurs extrêmes}
% graphs
% extremes vs non extremes
% indiv vs global
\pagebreak
\input{content/annexe/risque/h_glob_indiv/h_glob_vs_indiv_graphs.tex}



% \section{Étude de l'impact de la méthode de prélissage sur l'estimation de la régularité et le $\Delta$ optimal}
% \label{annexe:prelissage_impact}
% \input{content/annexe/risque/prelissage.tex}
\section{Lisser en utilisant une base de fonction sans écraser l'information irrégulière ?}
\label{annexe:lissage_base_fcn}

Le lissage spline donne une fonction de classe $\mathcal C^2$, ce qui est un désavantage dans le cadre du prélissage qui sert à déterminer les paramètres de régularité de courbes issues d'un processus que l'on ne suppose pas plus régulier que continu. Toutefois, le fait d'utiliser une base de fonctions pour effectuer le lissage a de nombreux avantages par rapport au lissage à noyaux qui peuvent éventuellement s'avérer utiles dans certaines situations spécifiques pour la mise en production de modèles.

En effet, une fois que l'on a déterminé les composantes de la décomposition de notre signal sur la base de fonctions, on n'a plus besoin de se référer aux données pour prédire une valeur. Il s'agit d'une méthode très économe en mémoire, ce qui peut être très avantageux dans le cadre de la mise en production de modèles lorsqu'il y a de nombreuses courbes observées.


\input{content/annexe/risque/prelissage_wavelet.tex}




% ! ——————————————————————————————————————————— !

\chapter{Un peu d'Histoire}
\label{annexe:histoire}
\section{ Histoire des séries temporelles }
\input{content/annexe/histoire/histoire_ts.tex}
\pagebreak
\section{ Histoire des données fonctionnelles }
\input{content/annexe/histoire/histoire_fda.tex}
\pagebreak
% \section{Histoire du mouvement brownien et de ses applications}
% \input{content/annexe/histoire/histoire_brownien.tex}
% \pagebreak

% ! ——————————————————————————————————————————— !

\chapter{Algorithmes \& Implémentations}

\label{annexe:code}
\section{Algorithmes de simulation}

\input{content/annexe/algo/algo.tex}

%\pagebreak
%\section{Implémentation R}
%\label{annexe:code-R}
%\input{content/annexe/algo/code.tex}

\chapter{Application}


\begin{figure}[H]
	\centering
	\begin{minipage}{0.32\textwidth}
		\centering
		6h
	\end{minipage}
	\begin{minipage}{0.32\textwidth}
		\centering
		6h30
	\end{minipage}
	\begin{minipage}{0.32\textwidth}
		\centering
		7h
	\end{minipage}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=0.32\textwidth]{Images/pv_pre/6h.png}
		\includegraphics[width=0.32\textwidth]{Images/pv_pre/06:30:00.jpg}
		\includegraphics[width=0.32\textwidth]{Images/pv_pre/07:00:00.jpg}
	\end{minipage}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=0.32\textwidth]{Images/pv_pre/20:30:00.jpg}
		\includegraphics[width=0.32\textwidth]{Images/pv_pre/21:00:00.jpg}
		\includegraphics[width=0.32\textwidth]{Images/pv_pre/21:30:00.jpg}
	\end{minipage}
	\begin{minipage}{0.32\textwidth}
		\centering
		20h30
	\end{minipage}
	\begin{minipage}{0.32\textwidth}
		\centering
		21h
	\end{minipage}
	\begin{minipage}{0.32\textwidth}
		\centering
		21h30
	\end{minipage}
	\caption{Distribution du facteur de charge l'ensemble des journées d'un parc photovoltaïque pendant la période estivale, observée à différentes heures}
	\label{fig:boxplot_pv_journee}

	\smallskip

	\emph{On constate qu'avant 6h30 et après 20h30, la distribution de la production électrique est telle que l'on peut la considérer nulle : des valeurs extrêmes de l'ordre de grandeur de $10^{-3}$ en facteur de charge et très concentrées autour d'une valeur presque nulle. Pendant les périodes de jour ( entre 6h30 et 20h30 ) on observe un étalement dans la distribution ainsi que des valeurs plus élevées de l'odre de grandeur de $10^{-2}$ à $10^{-1}$ pour le début de la journée.}

\end{figure}
